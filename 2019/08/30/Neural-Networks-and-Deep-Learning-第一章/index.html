<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/uploads/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/uploads/favicon-16x16.png">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sheldonleung.xyz","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="记录今天的读书笔记，《Neural Networks and Deep Learning》第一章">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks and Deep Learning 第一章">
<meta property="og:url" content="https://www.sheldonleung.xyz/2019/08/30/Neural-Networks-and-Deep-Learning-%E7%AC%AC%E4%B8%80%E7%AB%A0/index.html">
<meta property="og:site_name" content="Sheldon Leung&#39;s Bolg">
<meta property="og:description" content="记录今天的读书笔记，《Neural Networks and Deep Learning》第一章">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.1-1.png">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.3-1.png">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-1.png">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-2.png">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-3.png">
<meta property="og:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.5.1-1.png">
<meta property="article:published_time" content="2019-08-30T02:28:04.000Z">
<meta property="article:modified_time" content="2020-04-02T15:28:55.991Z">
<meta property="article:author" content="Sheldon Leung">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.1-1.png">

<link rel="canonical" href="https://www.sheldonleung.xyz/2019/08/30/Neural-Networks-and-Deep-Learning-%E7%AC%AC%E4%B8%80%E7%AB%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Neural Networks and Deep Learning 第一章 | Sheldon Leung's Bolg</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sheldon Leung's Bolg</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.sheldonleung.xyz/2019/08/30/Neural-Networks-and-Deep-Learning-%E7%AC%AC%E4%B8%80%E7%AB%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Sheldon Leung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon Leung's Bolg">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Neural Networks and Deep Learning 第一章
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-30 10:28:04" itemprop="dateCreated datePublished" datetime="2019-08-30T10:28:04+08:00">2019-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-02 23:28:55" itemprop="dateModified" datetime="2020-04-02T23:28:55+08:00">2020-04-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">读书笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>记录今天的读书笔记，《Neural Networks and Deep Learning》第一章</p>
<a id="more"></a>

<h2 id="1-1-感知器"><a href="#1-1-感知器" class="headerlink" title="1.1 感知器"></a>1.1 感知器</h2><h4 id="1-1-1-什么是感知器？"><a href="#1-1-1-什么是感知器？" class="headerlink" title="1.1.1 什么是感知器？"></a>1.1.1 什么是感知器？</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;感知器是一种人工神经元，在20世纪五、六十年代由科学家Frank Rosenblatt发明。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;一个感知器有一个或多个二进制输入($x_1,x_2,x_3$)，这些输入都有相应的<strong>权重</strong>($w_1,w_2,w_3$)，权重表示相应的输入对于输出有重要性的实数。感知器有一个二进制输出(0或者1)，感知器的输出则由分配权重后的信号量总和$\sum_jw_jx_j$⼩于或者⼤于<strong>阈值</strong>($threshold$)来决定。权重和阈值都是神经元的参数并且都是实数。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;神经元的代数形式：</p>
<p>$$<br>output=\begin{cases}<br>0 &amp;&amp; if \sum_jw_jx_j \leq threshold \\<br>1 &amp;&amp; if \sum_jw_jx_j \gt threshold<br>\end{cases}<br>$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;感知器的代数形式可以进一步简化，将信号量总和$\sum_j w_jx_j$化成点积$w \cdot x$，阈值($threshold$)化成偏置$b=-threshold$：</p>
<p>$$<br>output= \begin{cases}<br>0 &amp;&amp; if \quad w \cdot x + b \leq 0 \\<br>1 &amp;&amp; if \quad w \cdot x + b &gt; 0<br>\end{cases}<br>$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;在感知器中，任何权重或者偏置发生微小改变，都会引起输出发生巨大的改变。</p>
<p><img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.1-1.png" alt="感知器示意图"></p>
<p align="center">感知器示意图</p>

<h2 id="1-2-S型神经元"><a href="#1-2-S型神经元" class="headerlink" title="1.2 S型神经元"></a>1.2 S型神经元</h2><h3 id="1-2-1-S型神经元的工作原理"><a href="#1-2-1-S型神经元的工作原理" class="headerlink" title="1.2.1 S型神经元的工作原理"></a>1.2.1 S型神经元的工作原理</h3><ul>
<li>输入可以取0或者1之间的任意值</li>
<li>输出为$\sigma(w \cdot x + b)$，其中$\sigma(z) \equiv \frac{1}{1+e^{-z}}$，公式可写成$\frac{1}{1+exp(-\sum_jw_jx_j-b)}$</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;S型神经元是改进自感知器，在感知器的基础上引入偏导数，权重或者偏置做出的微小改动只引起输出的微小改动而不会引起输出的巨大改变。</p>
<p>$$<br>\Delta output \approx \sum_j \frac{\partial output}{\partial w_j}\Delta w_j + \frac{\partial output}{\partial b}\Delta b<br>$$</p>
<h2 id="1-3-神经网络的架构"><a href="#1-3-神经网络的架构" class="headerlink" title="1.3 神经网络的架构"></a>1.3 神经网络的架构</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;一个神经网络的结构主要由输入层，隐藏层和输出层构成。</p>
<p><img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.3-1.png" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;像这种多层网络结构的被称为多层感知器，但是构成网络的神经元不是感知器而是S型神经元。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;书中提到两种神经网络架构：</p>
<ol>
<li><p>前馈神经网络：将上一层的输出作为下一层的输入。上图就是一种前馈神经网络。</p>
</li>
<li><p>递归神经网络：不同于前馈神经网络的是，递归神经网络的结构中存在反馈环路。</p>
</li>
</ol>
<h2 id="1-4-一个简单的分类手写数字的网络"><a href="#1-4-一个简单的分类手写数字的网络" class="headerlink" title="1.4 一个简单的分类手写数字的网络"></a>1.4 一个简单的分类手写数字的网络</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;首先将图像<img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-1.png" alt="">分割成单独的$28 \times 28$的图像<img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-2.png" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;接下来使用一个三层神经网络来识别单个数字：</p>
<p><img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.4-3.png" alt="神经网络的结构"></p>
<p align="center">网络的结构</p>
&nbsp;&nbsp;&nbsp;&nbsp;网络的输入层包含给输入像素的值进行编码的784个神经元；网络的第二层是一个隐藏层(示例中只设置了15个神经元)；输出层包含10个神经元。

<p>&nbsp;&nbsp;&nbsp;&nbsp;为什么输出层为什么使用10个神经元而不适用4个神经元？</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;4个神经元每一个输出作为二进制的话，结果取决于它的输出更靠近0还是1，而且使用4个神经元一共有$2^4=16$种可能的输出。所以基于经验主义，使用10个神经元作为输出，识别效果更好</p>
<h2 id="1-5-使用梯度下降算法进行学习"><a href="#1-5-使用梯度下降算法进行学习" class="headerlink" title="1.5 使用梯度下降算法进行学习"></a>1.5 使用梯度下降算法进行学习</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;定义代价(损失)函数：</p>
<p>$$<br>C(w,b) \equiv \frac{1}{2n}\sum_x||y(x)-a||^2<br>$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;其中，$w$表示所有的网络中权重的集合，$b$是所有的偏置，$n$是训练输入数据的个数，$a$是表示当输入$x$时输出的向量，符号$||v||$是指向量$v$的模，$C$称为<strong>二次代价函数</strong>。</p>
<ul>
<li>目标：找出尽可能小的权重和偏置，使$C(w,b) \approx 0$。</li>
</ul>
<h3 id="1-5-1-双自变量"><a href="#1-5-1-双自变量" class="headerlink" title="1.5.1 双自变量"></a>1.5.1 双自变量</h3><p><img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/Neural-Networks-and-Deep-Learning/img1.5.1-1.png" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;想象有一个球在$v_1,v_2$方向分别移动很小的量，即$\Delta v_1$和$\large \Delta v_2$，球体的位置将会发生变化，通过微积分得到$C$的变化:</p>
<p>$$<br>\Delta C \approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2}\Delta v_2<br>$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;找出$\Delta v_1$和$\Delta v_2$使得$\Delta C$的值为负，定义一下两个式子：</p>
<p>$$<br>\Delta v \equiv (\Delta v_1,\Delta v_2)^T<br>\\<br>\nabla C \equiv (\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T<br>$$</p>
<p>​    &nbsp;&nbsp;&nbsp;&nbsp;其中，$\Delta v$为$v$变化的量，$\nabla C$表示梯度向量，$T$ 是转置符号。将以上三式重写，得到：</p>
<p>$$<br>\Delta C \approx \nabla C \cdot \Delta v<br>$$</p>
<p>​    &nbsp;&nbsp;&nbsp;&nbsp;在这个式子中，$\nabla C$把$v$的变化关联为$C$的变化，同时让我们知道如何选取$\Delta v$才能让$\Delta C$为负。</p>
<p>​    &nbsp;&nbsp;&nbsp;&nbsp;假设我们选取$\Delta v = -\eta\nabla C$，其中$\eta$是一个很小的正数(称为<strong>学习速率</strong>)。所以我们可以得到：</p>
<p>$$<br>\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta ||\nabla C||^2<br>\\<br>\because ||\nabla C||^2 \geq 0<br>\\<br>\therefore \Delta C \leq 0<br>$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;所以定义$\Delta v = -\eta\nabla C$为球体在梯度下降算法下的”运动定律”。也就是说我们使用方程$\Delta v = -\eta\nabla C$ 计算$\Delta v$来移动球体的位置，反复这样操作，持续减小$C$，最终会获得$C$的全局最小值。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;为了使$\Delta C \approx \nabla C \cdot \Delta v$达到很好的近似度，我们需要选择足够小的学习速率$\eta$，不然将会以$\eta \gt 0$结束；同时学习速率又不能过小，这样梯度下降算法就会运行得很慢。因此需要选择一个合适的学习速率能够达到很好的近似度，同时算法又不至于过慢。</p>
<h3 id="1-5-2-多自变量"><a href="#1-5-2-多自变量" class="headerlink" title="1.5.2 多自变量"></a>1.5.2 多自变量</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;假设$C$是一个有$m$个自变量的多元函数，那么$C$中的自变量变化为$\Delta v=(\Delta v_1,\cdots,\Delta v_m)^T$，则$\Delta C$为$\Delta C \approx \nabla C \cdot \Delta v$，梯度向量$\nabla C$为：$\nabla C \equiv (\frac{\partial C}{\partial v_1},\cdots,\frac{\partial C}{\partial v_m})^T$。与双自变量的情况类似，$\Delta v$我们可以选取$\Delta v = -\eta\nabla C$。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;假设我们正努力去改变$\Delta v$来让$\large C$尽可能地减小，这相当于最小化$\Delta C \approx \nabla C \cdot \Delta v$。首先限制步长为小的固定值，即$||\Delta v|| = \epsilon, \epsilon &gt; 0$。当步长固定时，我们要找到使得$C$减小最大的下降方向。可以证明，使得$\nabla C \cdot \Delta v$取得最小值的$\Delta v$为$\Delta v = -\eta\nabla C$，这里$\eta = \epsilon / ||\nabla C||$是有步长限制$||\Delta v|| = \epsilon$所决定的。因此，梯度下降法可以被视为一种在$C$下降最快的方向上做最微小变化的方法。</p>
<h3 id="1-5-3-如何在神经网络中使用梯度下降算法进行学习？"><a href="#1-5-3-如何在神经网络中使用梯度下降算法进行学习？" class="headerlink" title="1.5.3 如何在神经网络中使用梯度下降算法进行学习？"></a>1.5.3 如何在神经网络中使用梯度下降算法进行学习？</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;其思想就是利用梯度下降算法去寻找能够使得方程$C(w,b) \equiv \frac{1}{2n}\sum_x||y(x)-a||^2$取得最小值的权重$w_k$偏置$b_l$。具体过程根据 <strong>1.5.1 双自变量</strong>，将两个$v$分量替换成$w_k$和$b_l$，即可得到：</p>
<p>$$<br>\Delta w_k = -\eta \frac{\partial C}{\partial w_k}<br>\\<br>\Delta b_l = -\eta \frac{\partial C}{\partial b_l}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;剩下的思路都与1.5.1相似，这里就不在赘述。</p>
<h3 id="1-5-4-随机梯度下降算法"><a href="#1-5-4-随机梯度下降算法" class="headerlink" title="1.5.4 随机梯度下降算法"></a>1.5.4 随机梯度下降算法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;其思想就是通过随机选取小量的$m$个训练输入样本$X_1,X_2,\cdots,X_m$，这些称为<strong>小批量数据(mini-batch)</strong>。假设样本数量$m$足够大，我们期望$\nabla C_{X_j}$的平均值大致相等于整个$\nabla C_x$的平均值：<br>$$<br>\frac{\sum_{j=1}^{m}\nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;化简得到<br>$$<br>\nabla C \approx \frac{1}{m}\sum_{j=1}^m\nabla C_{X_j}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;将随机梯度下降算法和神经网络的学习联系起来，假设$w_k$和$b_l$表示我们神经网络中权重和偏置，即可得到：<br>$$<br>\Delta w_k = -\frac{\eta}{m}\sum_j\frac{\partial C_{X_j}}{\partial w_k}<br>\\<br>\Delta b_l = -\frac{\eta}{m}\sum_j\frac{\partial C_{X_j}}{\partial b_l}<br>$$</p>
<h2 id="1-6-实现我们的网络来分类数字"><a href="#1-6-实现我们的网络来分类数字" class="headerlink" title="1.6 实现我们的网络来分类数字"></a>1.6 实现我们的网络来分类数字</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在这里，我们将使用随机梯度下降算法和MNIST训练数据来编写一个识别手写数字的程序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        网络的构造函数.</span></span><br><span class="line"><span class="string">        :sizes: 各层神经元的数量,例如:[2,3,1].[list]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.num_layers = len(sizes)  <span class="comment"># 网络的层数</span></span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        <span class="comment"># 随机初始化偏置和权重并以Numpy矩阵列表的形式存储.</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :a: 网络给定的输入</span></span><br><span class="line"><span class="string">        :return: 网络输入对应的输出</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        随机梯度下降.</span></span><br><span class="line"><span class="string">        :training_data: 训练输入和其对应的期望输出[a list of tuples]</span></span><br><span class="line"><span class="string">        :epochs: 迭代次数</span></span><br><span class="line"><span class="string">        :mini_batch_size: 小批量数据的大小</span></span><br><span class="line"><span class="string">        :eta: 学习速率</span></span><br><span class="line"><span class="string">        :test_data: 验证数据</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)  <span class="comment"># 如果给出test_data,程序会在每次迭代之后评估网络</span></span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):  <span class="comment"># 在每个迭代期</span></span><br><span class="line">            random.shuffle(training_data)  <span class="comment"># 随机将训练数据打乱</span></span><br><span class="line">            mini_batches = [  <span class="comment"># 将数据分成多个大小适当的小批量数据</span></span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)  <span class="comment"># 对每一个mini_batch进行一次梯度下降</span></span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        梯度下降.根据单次梯度下降的迭代来更新网络的权重和偏置.</span></span><br><span class="line"><span class="string">        :mini_batch: 小批量数据 [a list of tuples]</span></span><br><span class="line"><span class="string">        :eta: 学习速率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)<span class="comment"># 这个函数大部分工作由这行代码完成</span></span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""S型函数."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>

<p>&nbsp;&nbsp;&nbsp;&nbsp;$w$表示矩阵，$w_{jk}$是连接第二层$k^{th}$神经元和第三层$j^{th}$神经元的权重。第三层神经元的激活向量是：<br>$$<br>{a}’=\sigma(wa+b)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;化成分量形式：<br>$$<br>\frac{1}{1+e^{-wa-b}}=\frac{1}{1 + exp(-\sum_j{w_j-b)}}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;<code>到这里就不在贴代码出来了，剩余的代码书上给的链接可以下载，并且里面的注释相当详细，可自行研究。(在读代码时有一些地方不必完全看懂，只要有一个概念，知道它是干嘛的就可以了，后期书上会讲到或者有需要可自己去查看相关资料)</code></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>如果你喜欢这篇文章，可以请我喝一杯 Coffee ~ (*^▽^*)</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/reward-QRcode/wechatpay.JPG" alt="Sheldon Leung 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://take-care.oss-cn-shenzhen.aliyuncs.com/blog-img/reward-QRcode/alipay.JPG" alt="Sheldon Leung 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2019/09/12/DHCP%E6%9C%8D%E5%8A%A1%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE/" rel="next" title="DHCP服务简单配置">
      DHCP服务简单配置 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-感知器"><span class="nav-text">1.1 感知器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-什么是感知器？"><span class="nav-text">1.1.1 什么是感知器？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-S型神经元"><span class="nav-text">1.2 S型神经元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-S型神经元的工作原理"><span class="nav-text">1.2.1 S型神经元的工作原理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-神经网络的架构"><span class="nav-text">1.3 神经网络的架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-一个简单的分类手写数字的网络"><span class="nav-text">1.4 一个简单的分类手写数字的网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-使用梯度下降算法进行学习"><span class="nav-text">1.5 使用梯度下降算法进行学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-1-双自变量"><span class="nav-text">1.5.1 双自变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-2-多自变量"><span class="nav-text">1.5.2 多自变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-3-如何在神经网络中使用梯度下降算法进行学习？"><span class="nav-text">1.5.3 如何在神经网络中使用梯度下降算法进行学习？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-4-随机梯度下降算法"><span class="nav-text">1.5.4 随机梯度下降算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-实现我们的网络来分类数字"><span class="nav-text">1.6 实现我们的网络来分类数字</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sheldon Leung"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Sheldon Leung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/sheldonleung" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sheldonleung" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:SheldonLeung593@outlook.com" title="E-Mail → mailto:SheldonLeung593@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://meteorkun.github.io/" title="https:&#x2F;&#x2F;meteorkun.github.io" rel="noopener" target="_blank">流星匆匆</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sheldon Leung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
